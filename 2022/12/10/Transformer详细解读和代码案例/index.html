<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta name="applicable-device" content="pc,mobile">
    <meta name="MobileOptimized" content="width"/>
    <meta name="HandheldFriendly" content="true"/>
    <!--Description-->
    
        <meta name="description" content="专注于数据开发, 数据分析, 数据采集, python教学">
    

    <!--Author-->
    
        <meta name="author" content="xxxspy">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Transformer详细解读和代码实现"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="专注于数据开发, 数据分析, 数据采集, python教学" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="DataSense"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Transformer详细解读和代码实现 - DataSense</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//cdn.bootcdn.net/ajax/libs/twitter-bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">

    <style>
    code{
    color: #d7ecff;
    font-size: 110%;
    }
    pre > code {
        -webkit-border-before: black;
        display: block;
        padding: 9.5px;
        margin: 0 0 10px;
        font-size: 13px;
        line-height: 1.42857143;
        color: #f9f4f4;
        word-break: break-all;
        word-wrap: break-word;
        background-color: #f5f5f500;
        border: 1px solid #eff3f521;
        border-radius: 0;
    }
    pre > code::before {
        content:"- ";
        z-index: -1;
        left:-2px;
    }
    #feedAv{position: fixed!important;left:-9999999px!important;z-index:-1000!important;}

    #MZAD_POP_PLACEHOLDER{position: fixed!important;left:-9999999px!important;z-index:-1000!important;}

    #pop_ad{position: fixed!important;left:-9999999px!important;z-index:-1000!important;}

    </style>

    
        <!-- mathjax -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>

    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true,
                processEnvironments: true
            },
            // Center justify equations in code and markdown cells. Elsewhere
            // we use CSS to left justify single line equations in code cells.
            displayAlign: 'center',
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}},
                linebreaks: { automatic: true }
            }
        });
        var maths = document.getElementsByTagName("code");
        MathJax.Hub.Queue(["Typeset",MathJax.Hub,maths]);
    </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    menuSettings: {
      zoom: "None"
    },
    showMathMenu: true,
    jax: ["input/TeX","output/CommonHTML"],
    extensions: ["tex2jax.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [["\\(", "\\)"],["$","$"]],
      displayMath: [["\\[", "\\]"],["$$", "$$"]],
      skipTags: ['^(?!code).*$']
    }
  });
  var maths = document.getElementsByTagName("code");
  MathJax.Hub.Queue(["Typeset",MathJax.Hub,maths]);
</script>

<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [ ['\\(','\\)']]} });
</script> -->

<!-- <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js"></script> -->
<!-- <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script> -->

    
    
    
    
    

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">
    <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i>
        
            <p id="main-title" class="title">DataSense</p>
        
        </a>
    </div>
</header>
<div class="page_list archive-container">
        
    <div class="archive-post" style="opacity: 1;">
        <a href="/2022/05/17/SPSS-AMOS数据分析案例教程-关于中介模型/">
            SPSS+AMOS数据分析案例教程-关于中介模
        </a>
    </div>
    <div class="archive-post" style="opacity: 1;">
        <a href="/2020/10/31/SPSS视频教程内容目录和跳转链接/">
            SPSS视频教程内容目录和跳转链接
        </a>
    </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/quarto/Doing-meta-analysis-in-R">
                Meta 分析入门教程
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/mplus-model-templates">
                Mplus中介和调节教程
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="https://mlln.cn/2019/12/26/2020%E5%B9%B4%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E9%97%AE%E5%8D%B7%E4%BA%92%E5%A1%AB%E7%BE%A4%E6%8E%A8%E8%8D%90/">
                大学生问卷互填群
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2024/11/12/Meta%E5%88%86%E6%9E%90%E4%BB%A3%E5%81%9A-%E4%BB%8E%E6%89%BE%E6%95%B0%E6%8D%AE%E5%88%B0%E5%87%BA%E6%8A%A5%E5%91%8A/">
                Meta分析辅导+代找数据
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2022/05/17/SPSS-AMOS数据分析案例教程-关于中介模型/">
                SPSS+AMOS数据分析案例教程-关于中介模
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2020/10/31/SPSS视频教程内容目录和跳转链接/">
                SPSS视频教程内容目录和跳转链接
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="https://mlln.cn/2022/08/11/R%E8%AF%AD%E8%A8%80%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E7%BB%83%E4%B9%A0%E6%9D%90%E6%96%99/">
                R语言快速入门视频教程
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2022/07/24/潜在类别分析入门/">
                LCA潜在类别分析和Mplus应用
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2022/06/12/Amos结构方程数据分析案例教程/">
                Amos结构方程模型数据分析入门教程
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="/2022/12/05/倒U关系回归分析中介效应和调节效应分析SPSS和R语言应用视频教程/">
                倒U关系回归分析中介效应和调节效应分析SPSS视频教程
            </a>
        </div>
    
        <div class="archive-post" style="opacity: 1;">
            <a href="./#xingqiu">
                统计咨询(图文问答)
            </a>
        </div>
    
</div>

    <!-- Main Content -->
    

    
        
<div class="row">
    
    <div class="col-md-9 col-sm-12">
        <!--Title and Logo-->
        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2022/12/10/Transformer%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB%E5%92%8C%E4%BB%A3%E7%A0%81%E6%A1%88%E4%BE%8B/">
                Transformer详细解读和代码实现
            </a>

        </h1>
        <div class="weibo-share">
            <p>在B站@mlln-cn, 我就能回答你的问题奥!</p>
        </div>
        <div class="post-info">
            
                <span class="date">2022年12月10日</span>
            
            
            
        </div>
    </div>

    <div class="content">
    <!-- Table of Contents -->

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82-Embedding"><span class="toc-number">1.</span> <span class="toc-text">嵌入层 Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-number">2.</span> <span class="toc-text">位置编码 Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%A6%E7%BC%A9%E6%94%BE%E7%9A%84%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Scaled-Dot-Product-Attention"><span class="toc-number">3.</span> <span class="toc-text">带缩放的点积注意力机制 Scaled Dot-Product Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8Cpadding%E6%8E%A9%E7%A0%81-Self-Attention-and-Padding-Mask"><span class="toc-number">4.</span> <span class="toc-text">自我注意力和padding掩码 Self-Attention and Padding Mask</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E8%BE%93%E5%85%A5%E7%9A%84%E5%90%8E%E7%BB%AD%E6%8E%A9%E7%A0%81-Subsequent-Mask-for-Decoder-Input"><span class="toc-number">5.</span> <span class="toc-text">解码器输入的后续掩码 Subsequent Mask for Decoder Input</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-Head-Attention"><span class="toc-number">6.</span> <span class="toc-text">多头注意力 Multi-Head Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E6%9C%BA%E5%88%B6-Position-wise-Feed-Forward"><span class="toc-number">7.</span> <span class="toc-text">分位置的前馈机制 Position-wise Feed-Forward</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-encoder"><span class="toc-number">8.</span> <span class="toc-text">编码器 encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EncoderBlock"><span class="toc-number">9.</span> <span class="toc-text">EncoderBlock</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-number">10.</span> <span class="toc-text">解码器 Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DecoderBlock"><span class="toc-number">11.</span> <span class="toc-text">DecoderBlock</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">12.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-Translator"><span class="toc-number">13.</span> <span class="toc-text">应用场景 Translator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">14.</span> <span class="toc-text">参考文献</span></a></li></ol>
    </div>

        <!-- Gallery -->
        

        <!-- Post Content -->
        <p>在本文中,我想讨论一下基于 Transformer 神经网络的代码细节，<br>因为很多人看了Transformer的理论架构以后还是云里雾里的，<br>所以你必须了解代码细节才能更深入的掌握Transformer架构。<br>这个架构是在这篇文章中提出《Attention Is All You Need》，  由Ashish Vaswani等人撰写。<br>Transformer已经有代码实现， 我已经在参考部分列出了其中一些。在阅读这些源代码时,我学习了一些技巧,这些技巧并未写在论文中，<br>所以我们想专门写一篇教程来介绍代码细节。</p>
<span id="more"></span>

<img src="imgs/transformerModel.png">

<p>在下文中,我使用PyTorch编写每个组件的细节。<br>最后,我将所有模块组装起来， 构成完整的Transformer。</p>
<h2 id="嵌入层-Embedding"><a href="#嵌入层-Embedding" class="headerlink" title="嵌入层 Embedding"></a>嵌入层 Embedding</h2><img src="imgs/Embedding.png">

<p>嵌入层的目的是将词索引转换为可计算的向量， 比如“你”这个词， 在词典中索引是5，<br>代表第五个词， 经过嵌入层后， 5转换为一个向量， 比如<code>$X = (0.1, 0.2, 0.11, -0.2, ..., 0.33)$</code></p>
<h2 id="位置编码-Positional-Encoding"><a href="#位置编码-Positional-Encoding" class="headerlink" title="位置编码 Positional Encoding"></a>位置编码 Positional Encoding</h2><img src="imgs/positionalEncoding.png">

<p>位置编码就是将词的位置信息转换为可计算的向量，<br>用 pos 代表词位置， i 代表维度，<br>我们可以使用如下公式计算位置编码：</p>
<img src="imgs/positionalEncodingFormular.png">

<p>我们可以用下面的代码来理解位置编码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pe = torch.zeros(max_positions, dim_embed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_positions):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, dim_embed, <span class="number">2</span>):</span><br><span class="line">        theta = pos / (<span class="number">10000</span> ** (i / dim_embed))</span><br><span class="line">        pe[pos, i    ] = math.sin(theta)</span><br><span class="line">        pe[pos, i + <span class="number">1</span>] = math.cos(theta)</span><br></pre></td></tr></table></figure>

<p>然而使用循环是非常耗时的， 所以我们利用矩阵的并行计算原理，<br>可以重写上面的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_positions: <span class="built_in">int</span>, dim_embed: <span class="built_in">int</span>, drop_prob: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> dim_embed % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Inspired by https://pytorch.org/tutorials/beginner/transformer_tutorial.html</span></span><br><span class="line">        position = torch.arange(max_positions).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        dim_pair = torch.arange(<span class="number">0</span>, dim_embed, <span class="number">2</span>)</span><br><span class="line">        div_term = torch.exp(dim_pair * (-math.log(<span class="number">10000.0</span>) / dim_embed))</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_positions, dim_embed)</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加batch维度</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 整个学习阶段， 位置信息是不变的， 注册为不可学习的数据</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">        self.dropout = nn.Dropout(p=drop_prob)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="comment"># 计算每个batch的最大句子长度</span></span><br><span class="line">        max_sequence_length = x.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 词向量中添加位置信息</span></span><br><span class="line">        x = x + self.pe[:, :max_sequence_length]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>这一行代码看起来有些复杂： <code>div_term = torch.exp(dim_pair * (-math.log(10000.0) / dim_embed))</code> ,<br>我们用数学公式写出来就不复杂了：</p>
<h2 id="带缩放的点积注意力机制-Scaled-Dot-Product-Attention"><a href="#带缩放的点积注意力机制-Scaled-Dot-Product-Attention" class="headerlink" title="带缩放的点积注意力机制 Scaled Dot-Product Attention"></a>带缩放的点积注意力机制 Scaled Dot-Product Attention</h2><img src="https://user-images.githubusercontent.com/8189217/209507887-41c38fd6-c2ac-46ba-838f-21b017c0ab56.png">

<p>带缩放的点积注意力机制顾名思义就是利用两个词向量的点乘来反映两者的相关性， 缩放（Scaled）意味着我们将点乘结果除以词向量维数的平方根。</p>
<p>这里的attention机制，相比于经典的Dot-product Attention其实就是多了一个scale项。这里的作用是啥呢？当d比较小的时候，要不要scale都无所谓，但是当d比较大时，内积的值的范围就会变得很大，不同的内积的差距也会拉大，这样的话，再经过softmax进一步的扩大差距，就会使得得到的attention分布很接近one-hot，这样会导致梯度下降困难，模型难以训练。在Transformer中，d&#x3D;512，算比较大了，因此需要进行scaling。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query: Tensor, key: Tensor, value: Tensor, mask: Tensor=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">    sqrt_dim_head = query.shape[-<span class="number">1</span>]**<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">    scores = scores / sqrt_dim_head</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask==<span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    </span><br><span class="line">    weight = F.softmax(scores, dim=-<span class="number">1</span>)    </span><br><span class="line">    <span class="keyword">return</span> torch.matmul(weight, value)</span><br></pre></td></tr></table></figure>

<h2 id="自我注意力和padding掩码-Self-Attention-and-Padding-Mask"><a href="#自我注意力和padding掩码-Self-Attention-and-Padding-Mask" class="headerlink" title="自我注意力和padding掩码 Self-Attention and Padding Mask"></a>自我注意力和padding掩码 Self-Attention and Padding Mask</h2><p>注意力机制就是产生一个权重矩阵， 如下图所示， 红色部分就是权重比较大的部分。<br><img src="https://user-images.githubusercontent.com/8189217/209510162-bb6fe82c-0abe-45ba-a30d-dc12f32805b5.png"></p>
<p>但是由于我们使用了Padding， 就是较短的句子会被空白符替代， 空白符不应该参与到权重计算中， 所以我们在mask中， 对于空白位置使用很大的负数，<br>这样经过softmask后， 权重趋近于0， 看起来权重矩阵就如下图：</p>
<img src="https://user-images.githubusercontent.com/8189217/209510166-477cfc32-2885-42b1-b5d7-ba5a3f318b29.png">

<h2 id="解码器输入的后续掩码-Subsequent-Mask-for-Decoder-Input"><a href="#解码器输入的后续掩码-Subsequent-Mask-for-Decoder-Input" class="headerlink" title="解码器输入的后续掩码 Subsequent Mask for Decoder Input"></a>解码器输入的后续掩码 Subsequent Mask for Decoder Input</h2><p>解码器的输入中， 我们使用mask来隐藏后续位置的信息。我们看下下面这个序列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&lt;SOS&gt;, &#x27;Bonjour&#x27;, &#x27;le&#x27;, &#x27;monde&#x27;, &#x27;!&#x27;]</span><br></pre></td></tr></table></figure>

<p>对于<code>&#39;Bonjour&#39;</code>， 不应该使用之后的信息<code> &#39;le&#39;, &#39;monde&#39;, &#39;!&#39;</code>, 所以它的注意力掩码就是<code>[1, 1, 0, 0, 0]</code>。<br>也就是说，在计算第一个位置的注意力值时,我们忽略第二个位置和之后的位置。在计算第二个位置的值时,我们忽略第三个位置和其余位置，以此类推：</p>
<img src="https://user-images.githubusercontent.com/8189217/209521736-b152f908-07dc-4b0d-a608-c60ac782c69f.png">

<p>所以， 解码器中的注意力机制是padding掩码和后续掩码的结合， 在生成的注意力矩阵中， 上三角是被掩盖的。</p>
<img src="https://user-images.githubusercontent.com/8189217/209511502-8567eae9-d199-4e0b-bb61-76522638e56c.png">

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_masks</span>(<span class="params">src_batch: Tensor, tgt_batch: Tensor</span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># [1] padding mask</span></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (batch_size, 1, max_tgt_seq_len)</span></span><br><span class="line">    src_pad_mask = (src_batch != PAD_IDX).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (batch_size, 1, max_src_seq_len)</span></span><br><span class="line">    tgt_pad_mask = (tgt_batch != PAD_IDX).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># [2] subsequent mask for decoder inputs</span></span><br><span class="line">    <span class="comment"># ----------------------------------------------------------------------</span></span><br><span class="line">    max_tgt_sequence_length = tgt_batch.shape[<span class="number">1</span>]</span><br><span class="line">    tgt_attention_square = (max_tgt_sequence_length, max_tgt_sequence_length)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># full attention</span></span><br><span class="line">    full_mask = torch.full(tgt_attention_square, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># subsequent sequence should be invisible to each token position</span></span><br><span class="line">    subsequent_mask = torch.tril(full_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add a batch dim (1, max_tgt_seq_len, max_tgt_seq_len)</span></span><br><span class="line">    subsequent_mask = subsequent_mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> src_pad_mask, tgt_pad_mask &amp; subsequent_mask</span><br></pre></td></tr></table></figure>

<h2 id="多头注意力-Multi-Head-Attention"><a href="#多头注意力-Multi-Head-Attention" class="headerlink" title="多头注意力 Multi-Head Attention"></a>多头注意力 Multi-Head Attention</h2><p>下图是原论文中用到的多头注意力机制图：</p>
<img src="https://user-images.githubusercontent.com/8189217/209523014-917b22de-5897-4ec2-9227-ab2c483d69f2.png">

<p>在编码器中， 多头注意力充当自我注意力机制( self-attention mechanism)， 解码器中充当自我注意力和目标源注意力(target-source attention), 可以看下图的区别：</p>
<p><img src="https://user-images.githubusercontent.com/8189217/209601652-a977429d-31a1-4249-8ee7-5c114020a0a2.png" alt="multiHeadAtt"></p>
<p>上图中， 在解码器中的“masked multi-head attention”， 多了一个mask， 因为解码器使用了mask来掩盖后续词的信息。<br>所以对于解码器， mask 是 padding mask 和 subsequent masks 的结合。</p>
<p>multi-head attention 有三个输入， 分别是V、K和Q， 在编码器中， 这三个都是词嵌入向量， 但在解码器中， V和K是编码器的输出结果，<br>Q是解码器的词嵌入向量。</p>
<p>从概念上讲,我们独立执行多个scaled dot-product attention计算, 每个头一个。<br>为此, 我们对 V, K, Q 执行8次独立的线性操作， 但是在实际情况中， 我们对这三个分别执行线性操作，<br>但之后将他们reshape成8个头， 正如下面代码所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># continuation of attention.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_heads: <span class="built_in">int</span>, dim_embed: <span class="built_in">int</span>, drop_prob: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> dim_embed % num_heads == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.dim_embed = dim_embed</span><br><span class="line">        self.dim_head = dim_embed // num_heads</span><br><span class="line"></span><br><span class="line">        self.query  = nn.Linear(dim_embed, dim_embed)</span><br><span class="line">        self.key    = nn.Linear(dim_embed, dim_embed)</span><br><span class="line">        self.value  = nn.Linear(dim_embed, dim_embed)</span><br><span class="line">        self.output = nn.Linear(dim_embed, dim_embed)</span><br><span class="line">        self.dropout = nn.Dropout(drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, y: Tensor, mask: Tensor=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        query = self.query(x)</span><br><span class="line">        key   = self.key  (y)</span><br><span class="line">        value = self.value(y)</span><br><span class="line"></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        query = query.view(batch_size, -<span class="number">1</span>, self.num_heads, self.dim_head)</span><br><span class="line">        key   = key  .view(batch_size, -<span class="number">1</span>, self.num_heads, self.dim_head)</span><br><span class="line">        value = value.view(batch_size, -<span class="number">1</span>, self.num_heads, self.dim_head)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Into the number of heads (batch_size, num_heads, -1, dim_head)</span></span><br><span class="line">        query = query.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        key   = key  .transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        value = value.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        attn = attention(query, key, value, mask)</span><br><span class="line">        attn = attn.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.dim_embed)</span><br><span class="line">        </span><br><span class="line">        out = self.dropout(self.output(attn))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>Q, K, V 的形状从<code>(batch_size, max_sequence_length, dim_embed)</code>变为<code>(batch_size, num_heads, max_sequence_length, dim_head)</code>，<br>而<code>dim_head = dim_embed // num_head</code>。 例如<code>dim_embed = 512 且 num_heads = 8</code>, 那么<code>dim_head = 64</code>。</p>
<p>然后， 我们将attention的结果reshape成<code>(batch_size, max_sequence_length, dim_embed)</code>。 所以， 经过了多头注意力机制，<br>输入输出的张量形状是相同的。</p>
<p><code>mask.unsqueeze(1)</code>的目的是给mask增加一个维度， 这样不同的头使用相同的mask， 在attention这个函数中， masked_fill可以实现这个目的。<br>最后， 使用<code>dropout</code>.</p>
<h2 id="分位置的前馈机制-Position-wise-Feed-Forward"><a href="#分位置的前馈机制-Position-wise-Feed-Forward" class="headerlink" title="分位置的前馈机制 Position-wise Feed-Forward"></a>分位置的前馈机制 Position-wise Feed-Forward</h2><p><img src="https://user-images.githubusercontent.com/8189217/209603830-e1c535f2-019d-401e-a6c3-1cdac2a2f806.png" alt="Position-wise-Feed-Forward"></p>
<p>Position-wise Feed-Forward 给词向量增加了非线性。 词向量的形状是<code>(batch_size, max_sequence_length, dim_embed)</code>,<br>很多神经网络处理词向量的时候会进行<code>flatten</code>然后再进入前馈神经网络， 我们并没有将词向量进行flatten， 我们的线性操作是对每个位置进行独立的操作，<br>因此， 这里被称为 Position-wise 。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim_embed: <span class="built_in">int</span>, dim_pffn: <span class="built_in">int</span>, drop_prob: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.pffn = nn.Sequential(</span><br><span class="line">            nn.Linear(dim_embed, dim_pffn),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(drop_prob),</span><br><span class="line">            nn.Linear(dim_pffn, dim_embed),</span><br><span class="line">            nn.Dropout(drop_prob),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">return</span> self.pffn(x)</span><br></pre></td></tr></table></figure>

<p>第一次线性操作扩大了维度。我了解这样做意味着ReLU不会丢失太多信息。<br>第二个线性操作将恢复原始维度。因此,我们可以继续使用相同的维度来执行其余过程。<br>例如在base Transformer模型中， 词嵌入向量从512扩展到2048， 然后再恢复到512 。</p>
<h2 id="编码器-encoder"><a href="#编码器-encoder" class="headerlink" title="编码器 encoder"></a>编码器 encoder</h2><p><img src="https://user-images.githubusercontent.com/8189217/209604556-fbbbc74e-aea7-4bc4-b0c8-5fb4c6074413.png" alt="encoder"></p>
<p>Transformer使用了多个encoder模块， 从图中可以看到。</p>
<p>下面的代码实现了多个encoder的堆叠， 但是没有实现<code>EncoderBlock</code>， 之后才会实现。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> .attention <span class="keyword">import</span> MultiHeadAttention</span><br><span class="line"><span class="keyword">from</span> .feed_forward <span class="keyword">import</span> PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_heads:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_embed:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_pffn:   <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 drop_prob:  <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.blocks = nn.ModuleList(</span><br><span class="line">            [EncoderBlock(num_heads, dim_embed, dim_pffn, drop_prob)</span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks)]</span><br><span class="line">        )</span><br><span class="line">        self.layer_norm = nn.LayerNorm(dim_embed)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, x_mask: Tensor</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</span><br><span class="line">            x = block(x, x_mask)</span><br><span class="line">        x = self.layer_norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="EncoderBlock"><a href="#EncoderBlock" class="headerlink" title="EncoderBlock"></a>EncoderBlock</h2><p><img src="https://user-images.githubusercontent.com/8189217/209604886-5a088875-a25e-475f-a68c-94cceff427da.png" alt="EncoderBlock"></p>
<p>接下来我们实现图中所示的结构。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># continuation of encoder.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_embed: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_pwff:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 drop_prob: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Self-attention</span></span><br><span class="line">        self.self_atten = MultiHeadAttention(num_heads, dim_embed, drop_prob)</span><br><span class="line">        self.layer_norm1 = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Point-wise feed-forward</span></span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(dim_embed, dim_pwff, drop_prob)</span><br><span class="line">        self.layer_norm2 = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, x_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        // 图中Add节点</span><br><span class="line">        x = x + self.sub_layer1(x, x_mask)</span><br><span class="line">        x = x + self.sub_layer2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sub_layer1</span>(<span class="params">self, x: Tensor, x_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        // 先进行norm</span><br><span class="line">        x = self.layer_norm1(x)</span><br><span class="line">        x = self.self_atten(x, x, x_mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sub_layer2</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.layer_norm2(x)</span><br><span class="line">        x = self.feed_forward(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>实际上代码和上面的概念图有区别， 因为有<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">研究</a>认为， 在进入sub layer之前先进行 normalization 更好。</p>
<p>所以我们采用下面的概念图：</p>
<p><img src="https://user-images.githubusercontent.com/8189217/209607298-b7aaa98c-61b0-4d95-9471-068e5093ed40.png" alt="EncoderBlock2"></p>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h2><p>我们先来实现总体流程：</p>
<p><img src="https://user-images.githubusercontent.com/8189217/209892281-45351e6b-e3b6-45dc-9c2b-2b08c0557904.png" alt="decoder"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> .attention <span class="keyword">import</span> MultiHeadAttention</span><br><span class="line"><span class="keyword">from</span> .feed_forward <span class="keyword">import</span> PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_blocks: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_heads:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_embed:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_pffn:   <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 drop_prob:  <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        // DecoderBlock 还没有实现  下面会写</span><br><span class="line">        self.blocks = nn.ModuleList(</span><br><span class="line">            [DecoderBlock(num_heads, dim_embed, dim_pffn, drop_prob)</span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks)]</span><br><span class="line">        )</span><br><span class="line">        self.layer_norm = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, x_mask: Tensor, y: Tensor, y_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self.blocks:</span><br><span class="line">            y = block(y, y_mask, x, x_mask)</span><br><span class="line">        y = self.layer_norm(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<h2 id="DecoderBlock"><a href="#DecoderBlock" class="headerlink" title="DecoderBlock"></a>DecoderBlock</h2><p>这里是DecoderBlock的具体细节：</p>
<p><img src="https://user-images.githubusercontent.com/8189217/209892377-86b66f89-851e-47ae-8f73-31adbfba6803.png" alt="DecoderBlock"></p>
<p>DecoderBlock有4个注意点：</p>
<ul>
<li>自我注意力使用 Masked multi-head attention</li>
<li>目标源注意力使用 multi-head attention</li>
<li>使用Position-wise feed-forward 给网络加入非线性</li>
<li>层之间使用 residual connection 和 layer normalization ：<code>x + Sublayer(LayerNorm(x))</code></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 num_heads: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_embed: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_pwff:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 drop_prob: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Self-attention</span></span><br><span class="line">        self.self_attn = MultiHeadAttention(num_heads, dim_embed, drop_prob)</span><br><span class="line">        self.layer_norm1 = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Target-source</span></span><br><span class="line">        self.tgt_src_attn = MultiHeadAttention(num_heads, dim_embed, drop_prob)</span><br><span class="line">        self.layer_norm2 = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Position-wise</span></span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(dim_embed, dim_pwff, drop_prob)</span><br><span class="line">        self.layer_norm3 = nn.LayerNorm(dim_embed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, y, y_mask, x, x_mask</span>) -&gt; Tensor:</span><br><span class="line">        // 实现residual connection</span><br><span class="line">        y = y + self.sub_layer1(y, y_mask)</span><br><span class="line">        y = y + self.sub_layer2(y, x, x_mask)</span><br><span class="line">        y = y + self.sub_layer3(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sub_layer1</span>(<span class="params">self, y: Tensor, y_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        y = self.layer_norm1(y)</span><br><span class="line">        y = self.self_attn(y, y, y_mask)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sub_layer2</span>(<span class="params">self, y: Tensor, x: Tensor, x_mask: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        y = self.layer_norm2(y)</span><br><span class="line">        y = self.tgt_src_attn(y, x, x_mask)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sub_layer3</span>(<span class="params">self, y: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        y = self.layer_norm3(y)</span><br><span class="line">        y = self.feed_forward(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>

<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p><img src="https://user-images.githubusercontent.com/8189217/209893866-18a66371-bbb9-4da7-ac9c-a2924e205ff4.png" alt="transformer"></p>
<p>下面就是将所有模块整合成一个模块的时候了， 所以下面的代码用到了以上所有的代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> ..modules <span class="keyword">import</span> Embedding, PositionalEncoding, Encoder, Decoder</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 input_vocab_size:  <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 output_vocab_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 max_positions:     <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_blocks:        <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 num_heads:         <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_embed:         <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 dim_pffn:          <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 drop_prob:         <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Input embeddings, positional encoding, and encoder</span></span><br><span class="line">        self.input_embedding = Embedding(input_vocab_size, dim_embed)</span><br><span class="line">        self.input_pos_encoding = PositionalEncoding(</span><br><span class="line">                                      max_positions, dim_embed, drop_prob)</span><br><span class="line">        self.encoder = Encoder(num_blocks, num_heads, dim_embed, dim_pffn, drop_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output embeddings, positional encoding, decoder, and projection </span></span><br><span class="line">        <span class="comment"># to vocab size dimension</span></span><br><span class="line">        self.output_embedding = Embedding(output_vocab_size, dim_embed)</span><br><span class="line">        self.output_pos_encoding = PositionalEncoding(</span><br><span class="line">                                       max_positions, dim_embed, drop_prob)</span><br><span class="line">        self.decoder = Decoder(num_blocks, num_heads, dim_embed, dim_pffn, drop_prob)</span><br><span class="line">        self.projection = nn.Linear(dim_embed, output_vocab_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize parameters</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> param.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(param)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor, y: Tensor,</span></span><br><span class="line"><span class="params">                      x_mask: Tensor=<span class="literal">None</span>, y_mask: Tensor=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        x = self.encode(x, x_mask)</span><br><span class="line">        y = self.decode(x, y, x_mask, y_mask)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x: Tensor, x_mask: Tensor=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        x = self.input_embedding(x)</span><br><span class="line">        x = self.input_pos_encoding(x)</span><br><span class="line">        x = self.encoder(x, x_mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, x: Tensor, y: Tensor,</span></span><br><span class="line"><span class="params">                     x_mask: Tensor=<span class="literal">None</span>, y_mask: Tensor=<span class="literal">None</span></span>) -&gt; Tensor:</span><br><span class="line">        y = self.output_embedding(y)</span><br><span class="line">        y = self.output_pos_encoding(y)</span><br><span class="line">        y = self.decoder(x, x_mask, y, y_mask)</span><br><span class="line">        <span class="keyword">return</span> self.projection(y)</span><br></pre></td></tr></table></figure>

<p>最后一层使用 nn.Linear 将词向量的维数转换为output_vocab_size， 这样就可以使用softmax输出词的概率。</p>
<h2 id="应用场景-Translator"><a href="#应用场景-Translator" class="headerlink" title="应用场景 Translator"></a>应用场景 Translator</h2><p>我们列举一个使用场景， 以便你对Transformer有更深的理解，下面是一个翻译器的架构：</p>
<p><img src="https://user-images.githubusercontent.com/8189217/209894595-393982c6-0ebd-408f-ac91-e2e6dbe73de6.png" alt="translater"></p>
<p>关键的流程如下：</p>
<ul>
<li>编码器从句子中提取特征</li>
<li>解码器的第一个输入是SOS(the start-of-sentence token)</li>
<li>解码器输出第一个词的概率</li>
<li>最大概率的词的词向量作为解码器的第二个输入</li>
<li>重复上面两个步骤</li>
<li>当输出EOS (end-of-sentence)时， 停止</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://kikaben.com/transformers-coding-details/#chapter-1">https://kikaben.com/transformers-coding-details/#chapter-1</a></li>
<li><a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1722622890411546852&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1722622890411546852&amp;wfr=spider&amp;for=pc</a></li>
<li><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></li>
</ul>
<blockquote>
<p><strong>注意</strong><br>深度学习和统计咨询请加QQ 2726725926, 微信 shujufenxidaizuo<br>请记住我的网址: mlln.cn 或者 jupyter.cn</p>
</blockquote>

    </div>

    

    
        <div class="post-tags">
            <i class="fa fa-tags" aria-hidden="true"></i>
            <a href="/tags/深度学习/">#深度学习</a>
        </div>
    



</div>
        </section>
        <section>
                <div class="content" style="color: #d7ecff">
                <h3 id="xingqiu"><a href="#星球" class="headerlink" title="星球"></a>统计咨询</h3>
                <p>统计咨询请加入我的星球,有问必回</p> 
                <p>加入星球向我提问(必回),下载资料,数据,软件等</p>
                <p><img src="/img/知识星球优惠券.png" class="img-thumbnail" style="width: 238px; height: 302px;"><img src="/img/知识星球.jpg" class="img-thumbnail" style="width: 238px; height: 302px;"></p>
            </div>
        </section>
        <section>
            <div class="content" style="color: #d7ecff">
                <h3 id="赞助"><a href="#赞助" class="headerlink" title="赞助"></a>赞助</h3>
                <p>持续创造有价值的内容, 我需要你的帮助</p>
                <p><img src="/img/收款二维码.png" class="img-thumbnail"></p>
            </div>
        </section>


    </div>

    <!-- rightside -->
    <div class="col-md-3 col-sm-12 rightside">
        <div class="related-posts">
    <h3>
        <a href="#">
            赞助推荐
        </a>
    </h3>
    <div class="page_list">
        
            <div class="archive-post">
                <a href="/mplus-model-templates">Mplus 中介和调节教程PPT</a>
            </div>
        
            <div class="archive-post">
                <a href="https://mlln.cn/2019/12/26/2020%E5%B9%B4%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E9%97%AE%E5%8D%B7%E4%BA%92%E5%A1%AB%E7%BE%A4%E6%8E%A8%E8%8D%90/">大学生问卷互填群</a>
            </div>
        
            <div class="archive-post">
                <a target="_blank" rel="noopener" href="https://item.taobao.com/item.htm?spm=2013.1.w4023-21724689242.4.76085e0a2k46Ky&amp;id=601314051411">问卷代填保证质量</a>
            </div>
        
    </div>
</div>
<div class="related-posts">
    <h3>
        常用工具
    </h3>
    <div class="page_list">
        <div class="archive-post">
            <a href="/2023/07/08/spss数据sav格式转换器（在线）/">Excel和SPSS转Mplus数据格式工具</a>
        </div>
        <div class="archive-post">
            <a href="/mplusoutput/">Mplus结果查看器</a>
        </div>
        <div class="archive-post">
            <a href="/drawio/">绘图软件</a>
        </div>
        <div class="archive-post">
            <a href="/2018/09/26/在线绘制中介效应图/">
                在线绘制中介效应图
            </a>
        </div>
        <div class="archive-post">
            <a href="/neural/">神经网络可视化</a>
        </div>
        <div class="archive-post">
            <a href="/psychopy/">psychopy教程</a>
        </div>
        <div class="archive-post">
            <a href="/vector/">[词向量]高维向量可视化</a>
        </div>
        <div class="archive-post">
            <a href="/demos/evolutionSimulator/">[tensorflow.js案例]进化算法演示</a>
        </div>
        <div class="archive-post">
            <a href="/guideToPython/">[书]Python最佳实践</a>
        </div>
        <div class="archive-post">
            <a href="/2017/10/10/sobel检验原理和一键计算器/">Sobel检验计算器</a>
        </div>
        <div class="archive-post">
            <a href="/2019/09/06/spss计算平均方差提取量AVE和组合信度CR的方法/">平均方差提取量AVE和组合信度CR计算器</a>
        </div>
        <div class="archive-post">
            <a href="/2017/12/12/JavaScript做的表格卡方检验-一键在线测试/">卡方检验计算器</a>
        </div>
        <div class="archive-post">
            <a href="/2020/03/22/SPSS做简单调节效应分析Model-1-Of-Process/">简单交互效应可视化工具</a>
        </div>
        <div class="archive-post">
            <a href="/2020/10/10/P值显著性计算器/">P值显著性计算器</a>
        </div>
        <div class="archive-post">
            <a href="/2020/12/23/均值差异的效应量在线计算器/">两组均值差异的效应量在线计算器</a>
        </div>
        <div class="archive-post">
            <a href="https://mlln.cn/2020/09/17/%E5%8D%A1%E6%96%B9%E5%B7%AE%E5%80%BC%E7%9A%84%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E9%AA%8C/">卡方差值的显著性检验</a>
        </div>
        
    </div>
</div>



<div class="related-posts">
    <div class="archive-post">
        <a href="#">Python数据科学技术交流QQ群:116384132</a>
    </div>
    <div class="archive-post">
        <a href="#">SPSS/Stata统计分析QQ群:572803384</a>
    </div>
    <div class="archive-post">
        <a href="#">如果你有其他有价值的群,想放到这里分享,可以联系我QQ675495787</a>
    </div>
</div>
<div class="related-posts">
    <h3>
        <a href="/tags/深度学习/">
            深度学习
        </a>
    </h3>
    <div class="page_list">
        
            <div class="archive-post">
                <a href="/2024/09/04/Deep-Live-Cam-Real-Time-Face-Swap-%E5%9C%A8-windows-%E4%B8%8A%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/">
                    Deep Live Cam Real-Time Face Swap 在 windows 上安装过程记录
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2022/12/10/Transformer%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB%E5%92%8C%E4%BB%A3%E7%A0%81%E6%A1%88%E4%BE%8B/">
                    Transformer详细解读和代码实现
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/08/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%B4%AA%E5%90%83%E8%9B%87%E6%9C%BA%E5%99%A8%E4%BA%BA-%E8%A7%86%E9%A2%91%E5%B1%95%E7%A4%BA/">
                    神经网络实现的贪吃蛇机器人-视频展示
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/08/26/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E8%AF%AD%E6%96%99%E5%BA%93%E5%A4%A7%E5%85%A8(%E5%85%8D%E8%B4%B9%E4%B8%8B)/">
                    机器翻译语料库大全(免费下)
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/08/23/AI%E5%AF%B9%E5%AF%B9%E8%81%94%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B/">
                    Ai对对联训练过程-好多精彩对联不舍得删
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/08/22/%E4%BD%BF%E7%94%A8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%8E%A9%E6%8E%92%E7%90%83%E6%B8%B8%E6%88%8F/">
                    使用循环神经网络和遗传算法玩排球游戏
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/07/05/Tensorflow%E9%80%90%E6%AD%A5%E5%AE%9E%E7%8E%B0%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C(Memory%20networks)%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/">
                    Tensorflow逐步实现记忆网络(Memory networks)第一部分
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/07/05/%E4%BD%9C%E6%96%87%E8%87%AA%E5%8A%A8%E8%AF%84%E5%88%86-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81position-encoding(PE)%E8%A7%A3%E9%87%8A/">
                    作文自动评分-位置编码position-encoding(PE)解释
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/07/04/Tensorflow.js%E5%AE%9E%E7%8E%B0%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%8F%8A%E5%8F%AF%E8%A7%86%E5%8C%96%E6%BC%94%E7%A4%BA/">
                    Tensorflow.js实现进化算法及可视化演示
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/07/02/tensorflow%E6%95%99%E7%A8%8B02-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%8F%8A%E5%85%B6%E5%AE%9E%E8%B7%B5/">
                    tensorflow教程02-计算图及其实践
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/07/02/tensorflow%E6%95%99%E7%A8%8B03-%E5%85%A5%E9%97%A8%E5%BF%85%E7%9F%A5%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/">
                    tensorflow教程03-入门必知数据模型和相关概念
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/06/21/tensorflow%E6%95%99%E7%A8%8B01-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%E5%92%8C%E7%AE%80%E5%8D%95%E6%A1%88%E4%BE%8B/">
                    tensorflow教程01-神经网络模型通俗理解和简单案例
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/03/10/Q-learning%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%8E%A9%E6%8E%A8%E7%AE%B1%E5%AD%90%E5%B0%8F%E6%B8%B8%E6%88%8F/">
                    Q-learning人工智能玩推箱子小游戏
                </a>
            </div>
            
            <div class="archive-post">
                <a href="/2018/02/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CBP%E7%AE%97%E6%B3%95%E6%B1%82%E5%AF%BC-%E6%89%8B%E5%8A%A8%E6%8E%A8%E5%AF%BC/">
                    神经网络BP算法求导-手动推导
                </a>
            </div>
            
    </div>
</div>


<div class="related-posts hidden-sm-down">
    <h3>
        <a href="#">
            友商赞助
        </a>
    </h3>
    <div class="page_list">
        <div class="archive-post">
            <a target="_blank" rel="noopener" href="http://jupyter.cn">
                  问卷大全问卷案例下载
            </a>
            <a href="https://mlln.cn/2019/12/26/2020%E5%B9%B4%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%E9%97%AE%E5%8D%B7%E4%BA%92%E5%A1%AB%E7%BE%A4%E6%8E%A8%E8%8D%90/">
                大学生互填问卷群推荐
            </a>
        </div>
    </div>
</div>
    </div>
</div>



</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-3 col-lg-3 footer-about">
                <h2>About</h2>
                <p>
                    这是<code>xxxspy</code>的个人博客
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2025/02/26/%E9%9A%8F%E6%9C%BA%E6%88%AA%E8%B7%9D%E4%BA%A4%E5%8F%89%E6%BB%9E%E5%90%8E%E9%9D%A2%E6%9D%BF%E6%A8%A1%E5%9E%8B%E9%99%84mplus%E4%BB%A3%E7%A0%81%E5%92%8Clavaan%E4%BB%A3%E7%A0%81/">随机截距交叉滞后面板模型附mplus代码和lava</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2025/02/24/%E5%A4%9A%E5%B1%82%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8BSPSS%E6%95%99%E7%A8%8B%E9%99%84%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B/">多层线性模型SPSS教程附视频教程</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2025/02/21/mplus%E5%81%9A%E6%B7%B7%E5%90%88%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8BGMM%E6%97%B6%E6%B2%A1%E6%9C%89%E8%BE%93%E5%87%BACFI%E7%AD%89%E6%8B%9F%E5%90%88%E6%8C%87%E6%A0%87/">mplus做混合增长模型GMM时没有输出CFI等拟</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2025/02/18/%E6%B7%B7%E5%90%88%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8BGMM%E6%A1%88%E4%BE%8B%E6%95%99%E7%A8%8B%E8%A7%A3%E8%AF%BBMplus%E4%BB%A3%E7%A0%81/">混合增长模型GMM案例教程解读Mplus代码</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2025/02/17/%E4%BD%BF%E7%94%A8%E6%BD%9C%E5%8F%98%E9%87%8F%E6%B7%B7%E5%90%88%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B%EF%BC%88GMM%EF%BC%89%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97/">使用潜变量混合增长模型（GMM）实用指南</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2025/02/04/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%8D%9A%E5%A3%AB%E6%AF%95%E4%B8%9A%E6%89%80%E6%9C%89%E6%95%99%E7%A8%8B%E4%BB%A3%E7%A0%81PPT%E8%A7%86%E9%A2%91%E8%B5%84%E6%BA%90%E4%B8%8B%E8%BD%BD/">计量经济学从入门到博士毕业所有教程代码PPT视频资</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2024/12/22/%E5%85%B1%E5%90%8C%E6%96%B9%E6%B3%95%E5%81%8F%E5%B7%AE%E7%9A%84%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/">共同方法偏差的类型和检验方法</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2024/11/12/Meta%E5%88%86%E6%9E%90%E4%BB%A3%E5%81%9A-%E4%BB%8E%E6%89%BE%E6%95%B0%E6%8D%AE%E5%88%B0%E5%87%BA%E6%8A%A5%E5%91%8A/">Meta分析-从找数据到出报告</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2024/11/07/SPSS%E5%81%9AMeta%E5%88%86%E6%9E%90%E5%8E%9F%E6%9D%A5%E8%BF%99%E4%B9%88%E7%AE%80%E5%8D%95-%E6%95%99%E7%A8%8B%E5%A6%82%E4%B8%8B/">SPSS做Meta分析原来这么简单-教程如下</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2024/11/07/SPSS%E5%81%9AMeta%E5%88%86%E6%9E%90%E5%8E%9F%E6%9D%A5%E8%BF%99%E4%B9%88%E7%AE%80%E5%8D%95-%E6%95%99%E7%A8%8B%E5%A6%82%E4%B8%8B/SPSS%E5%81%9AMeta%E5%88%86%E6%9E%90%E5%8E%9F%E6%9D%A5%E8%BF%99%E4%B9%88%E7%AE%80%E5%8D%95-%E6%95%99%E7%A8%8B%E5%A6%82%E4%B8%8B/">SPSS做Meta分析原来这么简单-教程如下</a>
            </li>
            
        </ul>
    </div>



            
<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6 footer-tags">
    <h2>tags</h2>
    <ul>
        
        <span>
            <a class="footer-post" href="/tags/%E6%8A%A5%E5%91%8A%E7%B3%BB%E7%BB%9F/">报告系统</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/">文本挖掘</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/spss/">spss</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E9%97%AE%E5%8D%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">问卷数据分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/amos/">amos</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E5%85%B1%E5%90%8C%E6%96%B9%E6%B3%95%E5%81%8F%E5%B7%AE/">共同方法偏差</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/python/">python</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/manim/">manim</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%95%99%E7%A8%8B/">教程</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E4%BF%A1%E5%BA%A6%E5%88%86%E6%9E%90/">信度分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/SPSS/">SPSS</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%95%88%E5%BA%A6%E5%88%86%E6%9E%90/">效度分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E9%97%AE%E5%8D%B7%E4%BA%92%E5%A1%AB/">问卷互填</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E9%97%AE%E5%8D%B7%E6%98%9F/">问卷星</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E9%97%AE%E5%8D%B7%E4%BB%A3%E5%A1%AB/">问卷代填</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/conda/">conda</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E9%97%AE%E5%8D%B7%E7%BD%91/">问卷网</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/apim/">apim</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/mplus/">mplus</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/numpy/">numpy</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/excel/">excel</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/datanitro/">datanitro</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/VBA/">VBA</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E7%BB%93%E6%9E%84%E6%96%B9%E7%A8%8B/">结构方程</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/smartpls/">smartpls</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%B8%B8%E6%88%8F/">游戏</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%8D%A2%E8%84%B8/">换脸</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/django/">django</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/EM/">EM</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/eprime/">eprime</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%8E%A8%E8%8D%90%E4%B9%A6/">推荐书</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/JSONSchema/">JSONSchema</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/">卡方检验</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/javascript/">javascript</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/jupyter/">jupyter</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/R/">R</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94/">调节效应</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/process/">process</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/MathJax/">MathJax</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Latex/">Latex</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/meta%E5%88%86%E6%9E%90/">meta分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Lpa/">Lpa</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%BD%9C%E5%9C%A8%E6%88%AA%E9%9D%A2%E5%88%86%E6%9E%90/">潜在截面分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/lca/">lca</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/mysql/">mysql</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%98%BE%E8%91%97%E6%80%A7/">显著性</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Q-learning/">Q-learning</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/R%E8%AF%AD%E8%A8%80/">R语言</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/r/">r</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Clinical-Decision-Making/">Clinical Decision Making</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Forest-Plot/">Forest Plot</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Effect-Size/">Effect Size</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/statsmodels/">statsmodels</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/tamplermonkey/">tamplermonkey</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/tensorflow-js/">tensorflow.js</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/tensorflow/">tensorflow</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E4%BD%9C%E6%96%87%E8%AF%84%E5%88%86/">作文评分</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/t%E6%A3%80%E9%AA%8C/">t检验</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/">方差分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/NLP/">NLP</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E8%AF%AD%E6%96%99%E5%BA%93/">语料库</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/nodejs/">nodejs</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/electron/">electron</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/e-prime/">e-prime</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/gensim/">gensim</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/">摘要生成</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/gitlab/">gitlab</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/git/">git</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/heckman/">heckman</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/gui/">gui</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/jamovi/">jamovi</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/notebook/">notebook</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/keras%E6%95%99%E7%A8%8B/">keras教程</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/keras%E5%9F%BA%E7%A1%80/">keras基础</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/mongodb/">mongodb</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/jieba/">jieba</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/mongoengine/">mongoengine</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/sem/">sem</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94/">中介效应</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/GMM/">GMM</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%B7%B7%E5%90%88%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/">混合增长模型</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E7%BA%B5%E5%90%91%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">纵向数据分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/Mplus/">Mplus</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E6%BD%9C%E5%9C%A8%E7%B1%BB%E5%88%AB%E5%88%86%E6%9E%90/">潜在类别分析</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/icc/">icc</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/cfa/">cfa</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/nginx/">nginx</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/networkx/">networkx</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/officejs/">officejs</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/p5-js/">p5.js</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/dataframe/">dataframe</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/pandas/">pandas</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/psychopy/">psychopy</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E4%B8%8B%E8%BD%BD/">下载</a>
        </span>
        
        <span>
            <a class="footer-post" href="/tags/%E7%BD%91%E7%9B%98/">网盘</a>
        </span>
        
    </ul>
</div>

            
        </div>
        <!-- links -->

<div class="row">
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 footer-about">
        <h2>Links</h2>
        <ul>
            
                <span> | </span><a class="footer-post" href="http://mlln.cn/baidusitemap.xml" target="_blank">网站地图</a><span> | </span>
            
                <span> | </span><a class="footer-post" href="http://mlln.cn/sitemap.xml" target="_blank">DataScience</a><span> | </span>
            
        </ul>
    </div>
</div>



        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a target="_blank" rel="noopener" href="https://github.com/xxxspy">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:675495787@qq.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @感谢github免费托管，hexo的静态网站引擎和模板主题制作者<a target="_blank" rel="noopener" href="http://www.codeblocq.com/">Jonathan Klughertz</a>
                    <br/>
                    <a target="_blank" rel="noopener" href="http://www.beian.miit.gov.cn">京ICP备16033873号-2</>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>

<!-- tweenmax -->
<script src="/js/TweenMax.min.js">

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- bootstrap.js -->
<script src="https://cdn.bootcss.com/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Baidu link push -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

<!-- baidu_static -->

	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?588f06b88af0ef575445f53432cd15ec";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>







</body>

</html>